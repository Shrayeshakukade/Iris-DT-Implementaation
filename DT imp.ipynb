{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"iris\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X = X.drop(\"species\", axis = 1)\n",
    "y = df[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data,output):\n",
    "        # data represents the feature upon which the node was split when fitting the training data\n",
    "        # data = None for leaf node\n",
    "        self.data = data\n",
    "        # children of a node are stored as a dicticionary with key being the value of feature upon which the node was split\n",
    "        # and the corresponding value stores the child TreeNode\n",
    "        self.children = {}\n",
    "        # output represents the class with current majority at this instance of the decision tree\n",
    "        self.output = output\n",
    "        # index will be used to assign a unique index to each node\n",
    "        self.index = -1\n",
    "        \n",
    "    def add_child(self,feature_value,obj):\n",
    "        self.children[feature_value] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        # root represents the root node of the decision tree built after fitting the training data\n",
    "        self.__root = None\n",
    "\n",
    "    def count_unique(self, Y):\n",
    "        # returns a dictionary with keys as unique values of Y(i.e no of classes) and the corresponding value as its frequency\n",
    "        d = {}\n",
    "        for i in Y:\n",
    "            if i not in d:\n",
    "                d[i]=1\n",
    "            else:\n",
    "                d[i]+=1\n",
    "        return d\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        # returns the entropy \n",
    "        freq_map = self.count_unique(Y)\n",
    "        entropy = 0\n",
    "        total = len(Y)\n",
    "        for i in freq_map:\n",
    "            p = freq_map[i]/total\n",
    "            entropy += (-p)*math.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def gain_ratio(self, X, Y, selected_feature):\n",
    "        # returns the gain ratio\n",
    "        info_orig = self.entropy(Y) # info_orig represents entropy before splitting\n",
    "        info_f = 0  # info_f represents entropy after splitting upon the selected feature\n",
    "        split_info = 0\n",
    "        values = set(X[:,selected_feature])\n",
    "        df = pd.DataFrame(X)\n",
    "        # Adding Y values as the last column in the dataframe \n",
    "        df[df.shape[1]] = Y\n",
    "        initial_size = df.shape[0] \n",
    "        for i in values:\n",
    "            df1 = df[df[selected_feature] == i]\n",
    "            current_size = df1.shape[0]\n",
    "            info_f += (current_size / initial_size) * self.entropy(df1[df1.shape[1] - 1])\n",
    "            split_info += (-current_size / initial_size) * math.log2(current_size / initial_size)\n",
    "\n",
    "        # to handle the case when split info = 0 which leads to division by 0 error\n",
    "        if split_info == 0 :\n",
    "            return math.inf\n",
    "\n",
    "        info_gain = info_orig - info_f\n",
    "        gain_ratio = info_gain / split_info\n",
    "        return gain_ratio\n",
    "\n",
    "    def gini_index(self,Y):\n",
    "        # returns the gini index \n",
    "        freq_map = self.count_unique(Y)\n",
    "        gini_index = 1\n",
    "        total = len(Y)\n",
    "        for i in freq_map:\n",
    "            p = freq_map[i] / total\n",
    "            gini_index -= p ** 2\n",
    "        return gini_index\n",
    "\n",
    "    def gini_gain(self, X, Y, selected_feature):\n",
    "        # returns the gini gain\n",
    "        gini_orig = self.gini_index(Y) # gini_orig represents gini index before splitting\n",
    "        gini_split_f = 0 # gini_split_f represents gini index after splitting upon the selected feature\n",
    "        values = set(X[:,selected_feature])\n",
    "        df = pd.DataFrame(X)\n",
    "        # Adding Y values as the last column in the dataframe \n",
    "        df[df.shape[1]] = Y\n",
    "        initial_size = df.shape[0] \n",
    "        for i in values:\n",
    "            df1 = df[df[selected_feature] == i]\n",
    "            current_size = df1.shape[0]\n",
    "            gini_split_f += (current_size / initial_size) * self.__gini_index(df1[df1.shape[1]-1])\n",
    "\n",
    "        gini_gain = gini_orig - gini_split_f\n",
    "        return gini_gain\n",
    "    \n",
    "    def decision_tree(self, X, Y, features, level, metric, classes):\n",
    "        # returns the root of the Decision Tree(which consists of TreeNodes) built after fitting the training data\n",
    "        # Here Nodes are printed as in PREORDER traversl\n",
    "        # classes represents the different classes present in the classification problem \n",
    "        # metric can take value gain_ratio or gini_index\n",
    "        # level represents depth of the tree\n",
    "        # We split a node on a particular feature only once (in a given root to leaf node path)\n",
    "        \n",
    "        \n",
    "        # If the node consists of only 1 class\n",
    "        if len(set(Y)) == 1:\n",
    "            print(\"Level\", level)\n",
    "            output = None\n",
    "            for i in classes:\n",
    "                if i in Y:\n",
    "                    output = i\n",
    "                    print(\"Count of\", i, \"=\", len(Y))\n",
    "                else :\n",
    "                    print(\"Count of\", i, \"=\", 0)\n",
    "            if metric == \"gain_ratio\":\n",
    "                print(\"Current Entropy is =  0.0\")\n",
    "            elif metric == \"gini_index\":\n",
    "                print(\"Current Gini Index is =  0.0\")\n",
    "\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None, output)\n",
    "\n",
    "        # If we have run out of features to split upon\n",
    "        # In this case we will output the class with maximum count\n",
    "        if len(features) == 0:\n",
    "            print(\"Level\", level)\n",
    "            freq_map = self.count_unique(Y)\n",
    "            output = None\n",
    "            max_count = -math.inf\n",
    "            for i in classes:\n",
    "                if i not in freq_map:\n",
    "                    print(\"Count of\", i, \"=\", 0)\n",
    "                else :\n",
    "                    if freq_map[i] > max_count :\n",
    "                        output = i\n",
    "                        max_count = freq_map[i]\n",
    "                    print(\"Count of\", i, \"=\", freq_map[i])\n",
    "\n",
    "            if metric == \"gain_ratio\":\n",
    "                print(\"Current Entropy  is =\", self.entropy(Y))\n",
    "            elif metric == \"gini_index\":\n",
    "                print(\"Current Gini Index is =\",self.gini_index(Y))            \n",
    "\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "\n",
    "        \n",
    "        # Finding the best feature to split upon\n",
    "        max_gain = - math.inf\n",
    "        final_feature = None\n",
    "        for f in features :\n",
    "            if metric == \"gain_ratio\":\n",
    "                current_gain = self.gain_ratio(X, Y, f)\n",
    "            elif metric ==\"gini_index\":\n",
    "                current_gain = self.gini_gain(X, Y, f)\n",
    "\n",
    "            if current_gain > max_gain:\n",
    "                max_gain = current_gain\n",
    "                final_feature = f\n",
    "\n",
    "        print(\"Level\", level)\n",
    "        freq_map = self.count_unique(Y)\n",
    "        output = None\n",
    "        max_count = - math.inf\n",
    "\n",
    "        for i in classes:\n",
    "            if i not in freq_map:\n",
    "                print(\"Count of\", i, \"=\", 0)\n",
    "            else :\n",
    "                if freq_map[i] > max_count :\n",
    "                    output = i\n",
    "                    max_count = freq_map[i]\n",
    "                print(\"Count of\", i, \"=\", freq_map[i])\n",
    "\n",
    "        if metric == \"gain_ratio\" :        \n",
    "            print(\"Current Entropy is =\", self.entropy(Y))\n",
    "            print(\"Splitting on feature X[\",final_feature,\"] with gain ratio \", max_gain, sep=\"\")\n",
    "            print()\n",
    "        elif metric == \"gini_index\":\n",
    "            print(\"Current Gini Index is =\",self.gini_index(Y))\n",
    "            print(\"Splitting on feature X[\",final_feature,\"] with gini gain \", max_gain, sep=\"\")\n",
    "            print()\n",
    "\n",
    "            \n",
    "        unique_values = set(X[:, final_feature]) # unique_values represents the unique values of the feature selected\n",
    "        df = pd.DataFrame(X)\n",
    "        # Adding Y values as the last column in the dataframe\n",
    "        df[df.shape[1]] = Y\n",
    "\n",
    "        current_node = TreeNode(final_feature, output)\n",
    "\n",
    "        # Now removing the selected feature from the list as we do not want to split on one feature more than once(in a given root to leaf node path)\n",
    "        index  = features.index(final_feature)\n",
    "        features.remove(final_feature)\n",
    "        for i in unique_values:\n",
    "            # Creating a new dataframe with value of selected feature = i\n",
    "            df1 = df[df[final_feature] == i]\n",
    "            # Segregating the X and Y values and recursively calling on the splits\n",
    "            node = self.decision_tree(df1.iloc[:, 0:df1.shape[1]-1].values, df1.iloc[:, df1.shape[1]-1].values, features, level+1, metric, classes)\n",
    "            current_node.add_child(i, node)\n",
    "\n",
    "        # Add the removed feature     \n",
    "        features.insert(index, final_feature)\n",
    "\n",
    "        return current_node\n",
    "    \n",
    "    def fit(self, X, Y, metric = \"gain_ratio\"):\n",
    "        # Fits to the given training data\n",
    "        # metric can take value gain_ratio or gini_index\n",
    "        features = [i for i in range(len(X[0]))]\n",
    "        classes = set(Y)\n",
    "        level = 0\n",
    "        if metric != \"gain_ratio\" :\n",
    "            if metric != \"gini_index\":\n",
    "                metric = \"gain_ratio\"  # if user entered a value which was neither gini_index nor gain_ratio\n",
    "        self.root = self.decision_tree(X, Y, features, level, metric, classes)\n",
    "        \n",
    "    def predict_for(self, data, node):\n",
    "        # predicts the class for a given testing point and returns the answer\n",
    "        \n",
    "        # We have reached a leaf node\n",
    "        if len(node.children) == 0 :\n",
    "            return node.output\n",
    "\n",
    "        val = data[node.data] # represents the value of feature on which the split was made       \n",
    "        if val not in node.children :\n",
    "            return node.output\n",
    "        \n",
    "        # Recursively call on the splits\n",
    "        return self.predict_for(data, node.children[val])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # This function returns Y predicted\n",
    "        # X should be a 2-D np array\n",
    "        Y = np.array([0 for i in range(len(X))])\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = self.predict_for(X[i], self.root)\n",
    "        return Y\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        # returns the mean accuracy\n",
    "        Y_pred = self.predict(X)\n",
    "        count = 0\n",
    "        for i in range(len(Y_pred)):\n",
    "            if Y_pred[i] == Y[i]:\n",
    "                count += 1\n",
    "        return count / len(Y_pred)\n",
    "    \n",
    "    def export_tree_pdf(self, filename = None):\n",
    "        # returns the tree as dot data\n",
    "        # if filename is specified the function \n",
    "        # will save the pdf file in current directory which consists of the visual reresentation of the tree\n",
    "        import pydotplus\n",
    "        from collections import deque\n",
    "        \n",
    "        dot_data = '''digraph Tree {node [shape=box] ;'''\n",
    "        \n",
    "        queue = deque()\n",
    "        \n",
    "        r = self.root\n",
    "        queue.append(r)\n",
    "        count = 0\n",
    "        if r.index == -1:\n",
    "            r.index = count\n",
    "        \n",
    "        dot_data = dot_data + \"\\n{} [label=\\\"Feature to split upon : X[{}]\\\\nOutput at this node : {}\\\" ];\".format(count, r.data, r.output) \n",
    "        \n",
    "        # Doing LEVEL ORDER traversal in the tree (using a queue)\n",
    "        while len(queue) != 0 :\n",
    "            node = queue.popleft()\n",
    "            for i in node.children:\n",
    "                count += 1\n",
    "                if(node.children[i].index == -1):\n",
    "                    node.children[i].index = count\n",
    "                \n",
    "                # Creating child node\n",
    "                dot_data = dot_data + \"\\n{} [label=\\\"Feature to split upon : X[{}]\\\\nOutput at this node : {}\\\" ];\".format(node.children[i].index, node.children[i].data, node.children[i].output) \n",
    "                # Connecting parent node with child\n",
    "                dot_data = dot_data + \"\\n{} -> {} [ headlabel=\\\"Feature value = {}\\\"]; \".format(node.index, node.children[i].index, i)\n",
    "                # Adding child node to queue\n",
    "                queue.append(node.children[i])\n",
    "        \n",
    "        dot_data = dot_data + \"\\n}\"\n",
    "\n",
    "        if filename != None:    \n",
    "            graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf(filename)    \n",
    "        \n",
    "        return dot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 1\n",
      "Count of 1 = 3\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature X[0] with gain ratio 0.31127812445913283\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 1\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature X[1] with gain ratio 1.0\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 1\n",
      "Count of 1 = 0\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Predictions : [0 1 1 1]\n",
      "\n",
      "Score : 1.0\n",
      "\n",
      "DOT DATA :- digraph Tree {node [shape=box] ;\n",
      "0 [label=\"Feature to split upon : X[0]\\nOutput at this node : 1\" ];\n",
      "1 [label=\"Feature to split upon : X[1]\\nOutput at this node : 0\" ];\n",
      "0 -> 1 [ headlabel=\"Feature value = 0\"]; \n",
      "2 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 2 [ headlabel=\"Feature value = 1\"]; \n",
      "3 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "1 -> 3 [ headlabel=\"Feature value = 0\"]; \n",
      "4 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "1 -> 4 [ headlabel=\"Feature value = 1\"]; \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# OR Tree\n",
    "clf1 = DecisionTreeClassifier()\n",
    "\n",
    "x = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "y = np.array([0,\n",
    "              1,\n",
    "              1,\n",
    "              1]) \n",
    "clf1.fit(x,y)\n",
    "Y_pred = clf1.predict(x)\n",
    "print(\"Predictions :\",Y_pred)\n",
    "print()\n",
    "print(\"Score :\",clf1.score(x, y)) # Score on training data\n",
    "print()\n",
    "print(\"DOT DATA :-\",clf1.export_tree_pdf(filename = \"tree_OR.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
